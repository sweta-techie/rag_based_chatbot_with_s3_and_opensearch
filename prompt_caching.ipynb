{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching with Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "!pip install anthropic -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import anthropic\nimport base64\nfrom IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Claude API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "api_key"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n\nCLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "upload_pdf"
   },
   "outputs": [],
   "source": [
    "def upload_pdf(path_to_pdf):\n    with open(path_to_pdf, \"rb\") as pdf_file:\n        binary_data = pdf_file.read()\n        base_64_encoded_data = base64.b64encode(binary_data)\n        base64_string = base_64_encoded_data.decode('utf-8')\n\n    return base64_string\n\n# Provide path to your PDF file\npath_to_pdf = 'BlackRock_investment-directions-q4-24-np-1-13.pdf'\npdf_data = upload_pdf(path_to_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Completion Method with Prompt Caching Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "completion_method"
   },
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\nMODEL_NAME = \"claude-3-5-sonnet-20241022\"\n\ndef get_completion(messages, model=MODEL_NAME):\n    completion = client.beta.messages.create(\n        betas=[\"pdfs-2024-09-25\", \"prompt-caching-2024-07-31\"],\n        model=model,\n        max_tokens=8192,\n        messages=messages,\n        temperature=0,\n    )\n    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Message with Prompt Caching Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "build_message"
   },
   "outputs": [],
   "source": [
    "def build_message_prompt_caching(query, pdf_data):\n    messages = [\n        {\n            \"role\": 'user',\n            \"content\": [\n                {\"type\": \"document\", \"source\": {\"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\": pdf_data}, \"cache_control\": {\"type\": \"ephemeral\"}},\n                {\"type\": \"text\", \"text\": query}\n            ]\n        }\n    ]\n\n    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Queries with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "send_queries"
   },
   "outputs": [],
   "source": [
    "queries = [\n    \"What are the main investment themes discussed in the Q4 2024 outlook?\",\n    \"What are the key takeaways from BlackRock's investment strategies for Q4 2024?\",\n    \"Which sectors are expected to benefit most from the AI build-out, according to BlackRock?\",\n    \"How might the upcoming U.S. presidential election impact investment strategies?\",\n    \"What are BlackRock's views on the impact of trade policies and economic fragmentation on inflation?\",\n    \"How does BlackRock suggest positioning a portfolio to mitigate geopolitical risks?\"]\n\n# Send queries with prompt caching\nfor idx, query in enumerate(queries[:1]):\n    print(f\"Query n° {idx+1}\")\n    messages = build_message_prompt_caching(query, pdf_data)\n    completion = get_completion(messages)\n    print(f\"\n--------ANSWER---------\n\")\n    print(completion.content[0].text)\n    print(f\"\n--------TOKENS COUNT---------\n\")\n    print(completion.usage)\n    print(f\"\n--------STOP REASON---------\n\")\n    print(completion.stop_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Costs with and without Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "compare_cost"
   },
   "outputs": [],
   "source": [
    "# Iterate through remaining queries\nfor idx, query in enumerate(queries[1:]):\n    print(f\"Query n° {idx+2}\")\n    messages = build_message_prompt_caching(query, pdf_data)\n    completion = get_completion(messages)\n    print(f\"\n--------ANSWER---------\n\")\n    print(completion.content[0].text)\n    print(f\"\n--------TOKENS COUNT---------\n\")\n    print(completion.usage)\n    print(f\"\n--------STOP REASON---------\n\")\n    print(completion.stop_reason)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
